from datetime import datetime, timedelta
import os
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.utils.helpers import chain
from airflow.operators.postgres_operator import PostgresOperator
from operators import (
    StageToRedshiftOperator,
    LoadFactOperator,
    LoadDimensionOperator,
    DataQualityOperator
)
from helpers import SqlQueries

# AWS_KEY = os.environ.get('AWS_KEY')
# AWS_SECRET = os.environ.get('AWS_SECRET')
default_args = {
    'owner': 'udacity',
    'start_date': datetime(2019, 1, 12),
    'depends_on_past': False,
    'email': 'airflow@example.com',
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)}


dag_create = DAG('dag_create_tables',
          default_args=default_args,
          description='Create tables',
          catchup=False
        )

create_artists = PostgresOperator(
    task_id='createtableartists',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    DROP TABLE IF EXISTS public.artists;
    CREATE TABLE public.artists (
	artistid varchar(256) NOT NULL,
	name varchar(256),
	location varchar(256),
	lattitude numeric(18,0),
	longitude numeric(18,0));
    COMMIT;
    """)

create_songplays = PostgresOperator(
    task_id='createtablesongplays',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    DROP TABLE IF EXISTS public.songplays;
    CREATE TABLE public.songplays (
	playid varchar(32) NOT NULL,
	start_time timestamp NOT NULL,
	userid int4 NOT NULL,
	"level" varchar(256),
	songid varchar(256),
	artistid varchar(256),
	sessionid int4,
	location varchar(256),
	user_agent varchar(256),
	CONSTRAINT songplays_pkey PRIMARY KEY (playid)
    );
    COMMIT;
    """)
    
create_songs = PostgresOperator(
    task_id='createtablesongs',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    DROP TABLE IF EXISTS public.songs;
    CREATE TABLE public.songs (
	songid varchar(256) NOT NULL,
	title varchar(256),
	artistid varchar(256),
	"year" int4,
	duration numeric(18,0),
	CONSTRAINT songs_pkey PRIMARY KEY (songid)
    );
    COMMIT;
    """)
        
create_staging_events = PostgresOperator(
    task_id='createtableevents',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    DROP TABLE IF EXISTS staging_events;
    CREATE TABLE public.staging_events (
	artist varchar(256),
	auth varchar(256),
	firstname varchar(256),
	gender varchar(256),
	iteminsession int4,
	lastname varchar(256),
	length numeric(18,0),
	"level" varchar(256),
	location varchar(256),
	"method" varchar(256),
	page varchar(256),
	registration numeric(18,0),
	sessionid int4,
	song varchar(256),
	status int4,
	ts int8,
	useragent varchar(256),
	userid int4
    );
    COMMIT;
    """)
            
create_staging_songs = PostgresOperator(
    task_id='createtablesongs',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    CREATE TABLE public.staging_songs (
	num_songs int4,
	artist_id varchar(256),
	artist_name varchar(256),
	artist_latitude numeric(18,0),
	artist_longitude numeric(18,0),
	artist_location varchar(256),
	song_id varchar(256),
	title varchar(256),
	duration numeric(18,0),
	"year" int4
    );
    COMMIT;
    """)
    
create_time = PostgresOperator(
    task_id='createsongs',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    DROP TABLE IF EXISTS public."time";
    CREATE TABLE public."time" (
    start_time timestamp NOT NULL,
	"hour" int4,
	"day" int4,
	week int4,
	"month" varchar(256),
	"year" int4,
	weekday varchar(256),
	CONSTRAINT time_pkey PRIMARY KEY (start_time)
    );
    COMMIT;
    """)
    
create_users = PostgresOperator(
    task_id='createusers',
    dag=dag_create,
    redshift_conn_id="redshift",
    sql="""
    BEGINN;
    DROP TABLE IF EXISTS public.users;
    CREATE TABLE public.users (
	userid int4 NOT NULL,
	first_name varchar(256),
	last_name varchar(256),
	gender varchar(256),
	"level" varchar(256),
	CONSTRAINT users_pkey PRIMARY KEY (userid)
    );
    COMMIT;
    """)



dag = DAG('dag_pipeline',
          default_args=default_args,
          description='Load and transform data in Redshift with Airflow',
          catchup=False)

start_operator = DummyOperator(task_id='Begin_execution',  dag=dag)



stage_events_to_redshift = StageToRedshiftOperator(
    task_id='stage_events',
    dag=dag,
    redshift_conn_id="redshift",
    aws_credentials_id="aws_credentials",
    table='staging_events',
    s3_bucket='udacity-dend',
    s3_key='log_data/2018/11/',
    jsonpath="s3://udacity-dend/log_json_path.json"
    #JSONPaths file= a mapping document that COPY will use to map and parse the JSON source data into the target
)

stage_songs_to_redshift = StageToRedshiftOperator(
    task_id='Stage_songs',
    dag=dag,
    redshift_conn_id="redshift",
    aws_credentials_id="aws_credentials",
    s3_bucket="udacity-dend",
    s3_key="song_data/A/A/A/",
    table="staging_songs"
)

load_songplays_table = LoadFactOperator(
    task_id='Load_songplays_fact_table',
    dag=dag,
    redshift_conn_id="redshift",
    query=SqlQueries.songplay_table_insert,   
)

load_user_dimension_table = LoadDimensionOperator(
    task_id='Load_user_dim_table',
    dag=dag,
    redshift_conn_id="redshift",
    sql_statement=SqlQueries.user_table_insert,
    table='users',
    app_data='True'
)

load_song_dimension_table = LoadDimensionOperator(
    task_id='Load_song_dim_table',
    dag=dag,
    redshift_conn_id="redshift",
    sql_statement=SqlQueries.song_table_insert,
    table='songs',
    app_data='True'
)

load_artist_dimension_table = LoadDimensionOperator(
    task_id='Load_artist_dim_table',
    dag=dag,
    redshift_conn_id="redshift",
    sql_statement=SqlQueries.artist_table_insert,
    table='artists',
    app_data='True'
)

load_time_dimension_table = LoadDimensionOperator(
    task_id='Load_time_dim_table',
    dag=dag,
    redshift_conn_id="redshift",
    sql_statement=SqlQueries.time_table_insert,
    table='time',
    app_data='True'
)

run_quality_checks = DataQualityOperator(
    task_id='Run_quality_checks',
    dag=dag,
    redshift_conn_id="redshift",
    li=["songplays", 
            "songs",
            "users",
            "artists",
            "time"],
    query="SELECT count(*) FROM {}",
    failure_value=0
)

end_operator = DummyOperator(task_id='Stop_execution',  dag=dag)

start_operator >> [stage_events_to_redshift, stage_songs_to_redshift] >> load_songplays_table >> [load_user_dimension_table, load_song_dimension_table,load_time_dimension_table, load_artist_dimension_table] >> run_quality_checks >> end_operator
